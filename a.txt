Of course. Here is the updated proposal, which hides specific implementation details, provides a more descriptive use case, and clarifies the configurable nature of the memory overflow strategies.

-----

# Proposal: Advanced Memory Management for Ballerina AI Agents

**Authors:** Sasindu Alahakoon
**Created:** 2025-09-17

-----

## 1\. Summary

This proposal introduces a comprehensive, multi-layered memory management framework for Ballerina AI Agents. To move beyond the limitations of a simple, volatile message window, this framework introduces a dual-memory system: a **Short-Term Memory (STM)** for managing the active conversation and a pluggable **Long-Term Memory (LTM)** for persistent knowledge. This will empower developers to build agents that are contextual, personalized, and capable of handling long, complex interactions with users.

-----

## 2\. Motivation

The current agent memory implementation in Ballerina provides a basic, fixed-size chat history. While functional for short exchanges, it has significant limitations for building sophisticated agents:

  * **Context Loss:** It operates on a "first-in, first-out" basis. Once the message window is full, the oldest messages are permanently deleted, regardless of their importance. A critical fact mentioned at the start of a long conversation is lost forever.
  * **No Persistence:** The memory is entirely in-memory, meaning the entire conversation history is lost if the agent restarts. This prevents the creation of agents that can resume conversations or remember users across sessions.
  * **Lack of Intelligence:** It treats all messages as a simple transcript. There is no mechanism to distill important facts, understand the semantic meaning of the conversation, or access a static knowledge base.

To build truly intelligent agents, we need a more advanced memory system that can intelligently preserve context, persist information, and manage different types of knowledge.

-----

## 3\. Description of the Proposed Solution

The proposed solution is centered around a **Short-Term Memory (STM) component** that acts as the primary orchestrator for an agent's memory. This module manages the immediate, active conversation history and is flexible enough to use either an in-memory map for transient sessions or a persistent database for durable conversations.

A key feature of this STM component is its intelligent handling of memory overflow. **It is not limited to a single behavior; instead, based on developer configuration, when the active conversation exceeds a token limit, the system can employ one of several strategies:**

  * **Summarize:** Use an LLM to create a concise summary of the older parts of the conversation, replacing many messages with one, thus preserving context while saving space.
  * **Trim/Delete:** Employ a simple strategy of removing the oldest messages to make space for new ones.
  * **Flush to LTM:** Move important information from the active conversation into specialized **Long-Term Memory (LTM)** blocks for persistent storage and future retrieval.

**Long-Term Memory (LTM)** is not a single entity but a collection of specialized, pluggable memory blocks designed for different types of knowledge:

  * A **static memory block** for storing unchanging, foundational knowledge (e.g., company policies, the agent's purpose).
  * An **extractive memory block** that uses an LLM to identify and pull key facts from a conversation (e.g., user's name, preferences) and stores them as structured data.
  * A **vector-based semantic memory block** that stores information as embeddings, allowing for the retrieval of information based on conceptual meaning rather than just keywords.

-----

## 4\. Concrete Use Case: HR Policy Chatbot

This use case demonstrates how the memory system creates a stateful and intelligent HR assistant.

### Full Conversation Flow

An employee, Sarah, needs to understand the company's remote work policy. The conversation unfolds as follows:

**User**: Hi, I'm Sarah from the Marketing team. I need to understand our remote work policy.
**Assistant**: Hello Sarah\! I'm here to help you with HR policies. Our remote work policy allows eligible employees to work remotely up to 3 days per week with manager approval. Would you like me to explain the eligibility criteria or application process?

**User**: What are the eligibility criteria?
**Assistant**: For remote work eligibility, you need to: 1. Have completed at least 6 months with the company 2. Have satisfactory performance ratings 3. Hold a role suitable for remote work (as determined by your manager) 4. Have necessary equipment and reliable internet connection. Are you meeting these criteria, Sarah?

**User**: Yes, I've been here for 2 years and have good performance reviews. How do I apply?
**Assistant**: Great\! Since you meet the criteria, here's the application process: 1. Submit Form HR-101 (Remote Work Application) 2. Get manager approval and signature... The form is available on the employee portal under "HR Forms".

**User**: What if my manager initially says no?
**Assistant**: If your manager declines your request, you have options: 1. Ask for specific feedback... 2. Propose a trial period... 3. Request mediation through HR...

**User**: No, just want to be prepared. What about equipment - does the company provide anything?
**Assistant**: Yes, the company provides essential equipment for remote work: - Laptop/desktop computer... - Ergonomic chair allowance up to $300...

**User**: Perfect\! One more question - can I work from abroad occasionally?
**Assistant**: Working from abroad has additional restrictions: - Maximum 2 weeks per year... - Must be approved 30 days in advance...

**User**: That's very helpful\! I think I have everything I need for now.
**Assistant**: Excellent\! I'm glad I could help you understand our remote work policy, Sarah. Remember to submit Form HR-101 when you're ready to apply.

### Scenario 1: Configuration with a Summarization Strategy

In this configuration, the memory system is set to summarize the conversation when it gets too long.

#### STM Before Summarization

The active memory holds the initial part of the conversation, approaching its token limit.

```json
[
  {"role": "user", "content": "Hi, I'm Sarah from the Marketing team...", "timestamp": "..."},
  {"role": "assistant", "content": "Hello Sarah! I'm here to help...", "timestamp": "..."},
  {"role": "user", "content": "What are the eligibility criteria?", "timestamp": "..."},
  {"role": "assistant", "content": "For remote work eligibility, you need to...", "timestamp": "..."},
  {"role": "user", "content": "Yes, I've been here for 2 years...", "timestamp": "..."},
  {"role": "assistant", "content": "Great! Since you meet the criteria...", "timestamp": "..."},
  {"role": "user", "content": "What if my manager initially says no?", "timestamp": "..."},
  {"role": "assistant", "content": "If your manager declines your request...", "timestamp": "..."}
]
```

#### STM After Summarization

The system automatically generates a summary, replaces the old messages, and preserves the conversational context efficiently.

```json
[
  {
    "role": "system",
    "content": "CONVERSATION SUMMARY: Sarah from Marketing team inquired about remote work policy. Key discussion points: Remote work allows 3 days/week with approval. Sarah meets eligibility criteria (2+ years experience, good performance). Discussed application process using Form HR-101, manager approval requirement, and options if manager declines (feedback, trial periods, HR mediation).",
    "timestamp": "..."
  },
  { "role": "user", "content": "No, just want to be prepared. What about equipment...", "timestamp": "..." },
  { "role": "assistant", "content": "Yes, the company provides essential equipment...", "timestamp": "..." }
]
```

### Scenario 2: Configuration with a "Flush to LTM" Strategy

Alternatively, if the agent is configured to flush data to Long-Term Memory, the system preserves detailed facts persistently.

#### Long-Term Memory (After Flushing)

As the conversation happens, key facts are extracted and stored in the appropriate LTM blocks.

```xml
<memory>
<static_memory>
  - Remote work: 3 days/week with approval.
  - International work: max 2 weeks/year, 30-day advance approval.
</static_memory>

<extractive_memory>
  - User: Sarah
  - Department: Marketing
  - Status: Eligible for remote work (2 years tenure, good performance).
</extractive_memory>

<vector_memory>
  - (Vector embeddings representing discussions on eligibility, application process, equipment, and international work policies).
</vector_memory>
</memory>
```

#### STM After Flushing

The STM is now cleared of older messages, keeping the active context window lean while ensuring no critical data is lost.

```json
[
  { "role": "user", "content": "Perfect! One more question - can I work from abroad occasionally?", "timestamp": "..." },
  { "role": "assistant", "content": "Working from abroad has additional restrictions...", "timestamp": "..." },
  { "role": "user", "content": "That's very helpful! I think I have everything I need for now.", "timestamp": "..." },
  { "role": "assistant", "content": "Excellent! I'm glad I could help you understand our remote work policy, Sarah...", "timestamp": "..." }
]
```

-----

## 5\. Developer Experience

A developer would interact with these components through a clear and declarative API. The goal is to make composing a sophisticated memory system intuitive. For example, initializing a memory system might look like this:

```ballerina
// Illustrative code showing the concept
// 1. Initialize the desired LTM blocks
staticMemory = new StaticMemory(...);
extractiveMemory = new ExtractiveMemory(...);
vectorMemory = new VectorMemory(...);

// 2. Define the overflow strategy, e.g., flush to LTM
overflowConfig = {
    strategy: "flush",
    tokenLimit: 4096,
    ltmBlocks: [extractiveMemory, vectorMemory]
};

// 3. Initialize the main memory component
agentMemory = new AgentMemory(config = overflowConfig);

// 4. Attach the configured memory to the agent
myAgent = new Agent(memory = agentMemory, ...);
```

-----

## 6\. Key Conceptual Components

The implementation will consist of the following key conceptual components:

  * A **Standard Memory Interface:** A common interface that all memory types will adhere to, ensuring interoperability.
  * **The STM Orchestrator:** The central component that manages active chat history and executes the configured overflow strategy.
  * **Pluggable LTM Blocks:** A suite of components for different knowledge types:
      * A **Static Knowledge Store** for foundational, read-only facts.
      * An **Extractive Knowledge Store** for pulling specific details from conversations.
      * A **Semantic Knowledge Store** for vector-based similarity searches.
  * **Configuration Models:** A set of clear, typed records that allow developers to easily define the behavior and limits of the memory system.
